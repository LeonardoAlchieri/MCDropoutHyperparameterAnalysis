# FIXED PARAMETERS
hidden_activation_type: "relu"
batch_size: 32
# length_scale defines the length scale for the L2 regularization term. Basically, how much you have to rescale the term.
length_scale: 0.0001  
starting_learning_rate: 0.001
learning_rate_decay: 1
learning_rate_epoch_rate: 100
num_epochs: 50
num_crossval_folds: 3
prediction_threshold: 0.5  # threshold for binary classification
random_seed: 42
layer_size: 1000
results_path: "./results_baseline.nosync/"
subsample_path: "./subsampled_tasks.csv"
num_torch_threads: 5

# HYPERPARAMETERS TO INVESTIGATE
dropout_rate_s: [0]
model_precision_s: [
    0.001,
]  # also known as "tau". Defines the L2 regularization term
num_mcdropout_iterations_s: [2]
num_layers_s: [1, 3, 5, 10]